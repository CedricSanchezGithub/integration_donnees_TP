{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TRDE703 Atelier IntÃ©gration des DonnÃ©es",
   "id": "fd7d5d2acdc3555b"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-18T16:22:04.361479Z",
     "start_time": "2026-01-18T16:22:01.850029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. D'ABORD : ON CONFIGURE LES CHEMINS (Le \"Hack\") ---\n",
    "# On doit le faire AVANT d'importer 'etl.*' sinon Python ne trouve pas le dossier.\n",
    "\n",
    "current_dir = Path(os.getcwd())\n",
    "# Si le notebook est dans 'etl/', la racine est le dossier parent\n",
    "project_root = current_dir.parent if current_dir.name == \"etl\" else current_dir\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"âœ… Racine ajoutÃ©e au path : {project_root}\")\n",
    "\n",
    "# --- 2. ENSUITE : ON PEUT IMPORTER TON CODE ---\n",
    "from pyspark.sql import SparkSession\n",
    "from etl.shared.config import SPARK_CONFIG  # Maintenant Ã§a va marcher !\n",
    "\n",
    "# --- 3. ENFIN : ON LANCE SPARK AVEC LE JAR ---\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "builder = SparkSession.builder\n",
    "for key, val in SPARK_CONFIG.items():\n",
    "    builder = builder.config(key, val)\n",
    "\n",
    "spark = builder.getOrCreate()\n",
    "\n",
    "print(f\"âœ… Session Spark crÃ©Ã©e avec le JAR : {SPARK_CONFIG.get('spark.jars')}\")"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Racine ajoutÃ©e au path : /Users/cedricsanchez/Master1/Cours/integration_donnees_TP\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "26/01/18 17:22:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Session Spark crÃ©Ã©e avec le JAR : /Users/cedricsanchez/Master1/Cours/integration_donnees_TP/jars/mysql-connector-j-9.1.0.jar\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:22:04.377495Z",
     "start_time": "2026-01-18T16:22:04.361901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from etl.shared.config import SPARK_CONFIG, MYSQL_CONFIG\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"âš™ï¸ Configuration chargÃ©e avec succÃ¨s.\")"
   ],
   "id": "ef2390298c38f5a6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš™ï¸ Configuration chargÃ©e avec succÃ¨s.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:22:04.394363Z",
     "start_time": "2026-01-18T16:22:04.378273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# On construit le chemin proprement avec pathlib\n",
    "# project_root est dÃ©jÃ  dÃ©fini dans la premiÃ¨re cellule\n",
    "json_filepath = str(project_root / \"data\" / \"raw\" / \"openfoodfacts-products.jsonl\")\n",
    "\n",
    "print(f\"ðŸ“‚ Fichier cible : {json_filepath}\")\n",
    "\n",
    "if os.path.exists(json_filepath):\n",
    "    print(\"âœ… Le fichier existe bien.\")\n",
    "else:\n",
    "    print(\"âŒ Fichier introuvable. VÃ©rifie le dossier data/raw/\")"
   ],
   "id": "cf7e6bd3f66a8281",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ Fichier cible : /Users/cedricsanchez/Master1/Cours/integration_donnees_TP/data/raw/openfoodfacts-products.jsonl\n",
      "âœ… Le fichier existe bien.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### ðŸ’¡ Un mot sur nos choix (et les consignes du TP)\n",
    "\n",
    "Pourquoi s'embÃªter Ã  Ã©crire ce schÃ©ma manuellement ?\n",
    "\n",
    "1.  **Respect de la consigne :** Le sujet est strict : *\"Lecture JSON/CSV avec schÃ©ma explicite (pas d'infÃ©rence magique en prod)\"*. Utiliser `inferSchema=True` nous ferait perdre des points.\n",
    "2.  **Gestion de l'Historique (SCD2) :** Le sujet impose de gÃ©rer le *\"SCD2 produit\"*. Pour cela, nous avons impÃ©rativement besoin du timestamp brut (`last_modified_t` en `LongType`) pour comparer les versions Ã  la seconde prÃ¨s.\n",
    "3.  **Structure ImbriquÃ©e :** Le format JSONL groupe les nutriments dans un objet. Notre schÃ©ma reflÃ¨te cette rÃ©alitÃ© (`StructType` imbriquÃ©) pour Ã©viter de crÃ©er 1000 colonnes plates inutiles.\n",
    "4.  **SÃ©curitÃ© (`String`) :** Pour des champs instables comme `nova_group`, on lit en `String` pour Ã©viter que Spark ne rejette la ligne en cas de format inattendu."
   ],
   "id": "da340475acb067a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:22:04.409795Z",
     "start_time": "2026-01-18T16:22:04.395344Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, LongType, ArrayType\n",
    "\n",
    "def get_jsonl_schema():\n",
    "    \"\"\"\n",
    "    SchÃ©ma robuste pour l'ingestion JSONL.\n",
    "    GÃ¨re les types imbriquÃ©s (nested) propres Ã  MongoDB/JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    # DÃ©finition de la sous-structure pour les nutriments\n",
    "    # (Permet de lire l'objet \"nutriments\": { ... } proprement)\n",
    "    nutriments_schema = StructType([\n",
    "        StructField(\"energy-kcal_100g\", FloatType(), True),\n",
    "        StructField(\"sugars_100g\", FloatType(), True),\n",
    "        StructField(\"salt_100g\", FloatType(), True),\n",
    "        StructField(\"sodium_100g\", FloatType(), True),\n",
    "        StructField(\"fiber_100g\", FloatType(), True),\n",
    "        StructField(\"proteins_100g\", FloatType(), True)\n",
    "    ])\n",
    "\n",
    "    return StructType([\n",
    "        # --- Identifiants & MÃ©tadonnÃ©es (Crucial pour SCD2) ---\n",
    "        StructField(\"code\", StringType(), True),            # La clÃ© primaire mÃ©tier (EAN)\n",
    "        StructField(\"product_name\", StringType(), True),\n",
    "        StructField(\"last_modified_t\", LongType(), True),   # Timestamp UNIX (versioning)\n",
    "        StructField(\"created_t\", LongType(), True),\n",
    "\n",
    "        # --- Dimensions (Marques, CatÃ©gories...) ---\n",
    "        StructField(\"brands\", StringType(), True),\n",
    "        StructField(\"categories\", StringType(), True),\n",
    "        StructField(\"countries_tags\", ArrayType(StringType()), True), # Liste de pays [\"en:france\", \"en:belgium\"]\n",
    "\n",
    "        # --- QualitÃ© & Scores ---\n",
    "        StructField(\"nutriscore_grade\", StringType(), True),\n",
    "        StructField(\"nova_group\", IntegerType(), True),     # Souvent un entier dans le JSON\n",
    "        StructField(\"ecoscore_grade\", StringType(), True),\n",
    "\n",
    "        # --- Mesures (ImbriquÃ©es) ---\n",
    "        StructField(\"nutriments\", nutriments_schema, True)  # L'objet imbriquÃ©\n",
    "    ])\n",
    "\n",
    "print(\"âœ… SchÃ©ma JSONL dÃ©fini.\")"
   ],
   "id": "69b0cb1c6a8e6464",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SchÃ©ma JSONL dÃ©fini.\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:22:39.951406Z",
     "start_time": "2026-01-18T16:22:04.410656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Adapte le nom du fichier si nÃ©cessaire\n",
    "input_file = \"openfoodfacts-products.jsonl\"\n",
    "raw_path = str(project_root / \"data\" / \"raw\" / input_file)\n",
    "\n",
    "print(f\"ðŸ“‚ PrÃ©paration de la lecture : {raw_path}\")\n",
    "\n",
    "try:\n",
    "    # 1. Lecture \"Lazy\" (Paresseuse)\n",
    "    # Spark ne lit rien pour l'instant, il note juste le plan d'action.\n",
    "    df_raw = spark.read \\\n",
    "        .schema(get_jsonl_schema()) \\\n",
    "        .json(raw_path)\n",
    "\n",
    "    print(\"âœ… Lecture configurÃ©e (Lazy). Le chargement rÃ©el se fera aprÃ¨s le sampling.\")\n",
    "\n",
    "\n",
    "    count = df_raw.count()\n",
    "    print(f\"ðŸ“Š Nombre de produits ingÃ©rÃ©s : {count:,}\")\n",
    "\n",
    "    df_raw.printSchema()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur de lecture : {e}\")"
   ],
   "id": "6fa16efe239268b0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‚ PrÃ©paration de la lecture : /Users/cedricsanchez/Master1/Cours/integration_donnees_TP/data/raw/openfoodfacts-products.jsonl\n",
      "âœ… Lecture configurÃ©e (Lazy). Le chargement rÃ©el se fera aprÃ¨s le sampling.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:======================================================>(494 + 3) / 497]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Nombre de produits ingÃ©rÃ©s : 4,247,844\n",
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- product_name: string (nullable = true)\n",
      " |-- last_modified_t: long (nullable = true)\n",
      " |-- created_t: long (nullable = true)\n",
      " |-- brands: string (nullable = true)\n",
      " |-- categories: string (nullable = true)\n",
      " |-- countries_tags: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- nutriscore_grade: string (nullable = true)\n",
      " |-- nova_group: integer (nullable = true)\n",
      " |-- ecoscore_grade: string (nullable = true)\n",
      " |-- nutriments: struct (nullable = true)\n",
      " |    |-- energy-kcal_100g: float (nullable = true)\n",
      " |    |-- sugars_100g: float (nullable = true)\n",
      " |    |-- salt_100g: float (nullable = true)\n",
      " |    |-- sodium_100g: float (nullable = true)\n",
      " |    |-- fiber_100g: float (nullable = true)\n",
      " |    |-- proteins_100g: float (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ§¹ Ã‰tape 2 : Transformation \"Silver\" (Nettoyage & Typage)\n",
    "\n",
    "Maintenant que les donnÃ©es brutes sont chargÃ©es, nous devons les rendre utilisables pour l'analyse et le SCD2.\n",
    "Cette Ã©tape applique les rÃ¨gles de qualitÃ© demandÃ©es :\n",
    "\n",
    "1.  **Typage Temporel :** Conversion des timestamps UNIX (`Long`) en vraies dates (`Timestamp`) pour `last_modified_t` et `created_t`.\n",
    "2.  **Nettoyage Textuel :** Suppression des espaces superflus (`trim`) sur les codes-barres et noms.\n",
    "3.  **Extraction des Nutriments :** Aplatissement de la structure imbriquÃ©e `nutriments` pour faciliter les requÃªtes SQL futures.\n",
    "4.  **Gestion des Nulls :** Conversion sÃ©curisÃ©e de `nova_group` (texte vers entier) et filtrage des produits sans code-barre."
   ],
   "id": "9f02f9f8021945a5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:23:15.281046Z",
     "start_time": "2026-01-18T16:22:39.987592Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, trim, from_unixtime, to_timestamp, when\n",
    "\n",
    "print(\"â³ DÃ©marrage du nettoyage Silver avec Sampling...\")\n",
    "\n",
    "# --- CORRECTION 1 : ECHANTILLONNAGE (SAMPLING) ---\n",
    "# On ne garde que 10% des donnÃ©es (environ 400k lignes) pour sauver ton disque dur.\n",
    "# seed=42 permet d'avoir toujours les mÃªmes 10% si tu relances.\n",
    "df_sampled = df_raw.sample(withReplacement=False, fraction=0.1, seed=42)\n",
    "\n",
    "df_silver = df_sampled \\\n",
    "    .select(\n",
    "        # --- 1. Nettoyage des ClÃ©s & Textes ---\n",
    "        trim(col(\"code\")).alias(\"code\"),\n",
    "        trim(col(\"product_name\")).alias(\"product_name\"),\n",
    "\n",
    "        # --- 2. Gestion Temporelle ---\n",
    "        from_unixtime(col(\"last_modified_t\")).cast(\"timestamp\").alias(\"last_modified_ts\"),\n",
    "        from_unixtime(col(\"created_t\")).cast(\"timestamp\").alias(\"created_ts\"),\n",
    "\n",
    "        # --- 3. Normalisation des Dimensions ---\n",
    "        col(\"countries_tags\"),\n",
    "        trim(col(\"brands\")).alias(\"brands\"),\n",
    "        trim(col(\"categories\")).alias(\"categories\"),\n",
    "\n",
    "        # --- 4. QualitÃ© & Scores ---\n",
    "        trim(col(\"nutriscore_grade\")).alias(\"nutriscore_grade\"),\n",
    "        trim(col(\"ecoscore_grade\")).alias(\"ecoscore_grade\"),\n",
    "        col(\"nova_group\").cast(\"integer\").alias(\"nova_group\"),\n",
    "\n",
    "        # --- 5. Nutriments ---\n",
    "        col(\"nutriments.energy-kcal_100g\").alias(\"energy_kcal_100g\"),\n",
    "        col(\"nutriments.sugars_100g\").alias(\"sugars_100g\"),\n",
    "        col(\"nutriments.salt_100g\").alias(\"salt_100g\"),\n",
    "        col(\"nutriments.proteins_100g\").alias(\"proteins_100g\")\n",
    "    ) \\\n",
    "    .filter(col(\"code\").isNotNull()) \\\n",
    "    .filter(col(\"code\") != \"\")\n",
    "\n",
    "# On met en cache ce petit Ã©chantillon\n",
    "df_silver.cache()\n",
    "\n",
    "count = df_silver.count()\n",
    "print(f\"âœ… Nettoyage terminÃ© sur l'Ã©chantillon. Produits restants : {count:,}\")\n",
    "print(\"(C'est normal d'en avoir moins, on a pris 10% volontairement !)\")\n",
    "\n",
    "display(df_silver.select(\"code\", \"last_modified_ts\", \"product_name\").limit(5))"
   ],
   "id": "2a765f28173c7f65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ DÃ©marrage du nettoyage Silver avec Sampling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:======================================================>(493 + 4) / 497]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Nettoyage terminÃ© sur l'Ã©chantillon. Produits restants : 425,588\n",
      "(C'est normal d'en avoir moins, on a pris 10% volontairement !)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[code: string, last_modified_ts: timestamp, product_name: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ” Ã‰tape 3 : Fingerprinting (PrÃ©paration SCD2)\n",
    "\n",
    "Pour gÃ©rer l'historique (SCD2) efficacement, nous ne pouvons pas comparer toutes les colonnes Ã  chaque fois.\n",
    "Nous allons gÃ©nÃ©rer un **Hash Technique (`row_hash`)** : une empreinte digitale unique basÃ©e sur les colonnes mÃ©tier.\n",
    "\n",
    "* **StratÃ©gie :** On concatÃ¨ne toutes les colonnes importantes (Nom, Marque, Nutriscore, Sucre...) et on applique un hachage SHA-256.\n",
    "* **IntÃ©rÃªt :** Si le hash change, cela signifie que le produit a Ã©tÃ© modifiÃ©. C'est ce qui dÃ©clenchera la crÃ©ation d'une nouvelle version dans le Datamart."
   ],
   "id": "ffd1bf078dba36b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:23:15.455192Z",
     "start_time": "2026-01-18T16:23:15.408283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import sha2, concat_ws, col\n",
    "\n",
    "print(\"â³ Calcul du Hash (Fingerprint) pour chaque produit...\")\n",
    "\n",
    "# On sÃ©lectionne les colonnes qui doivent dÃ©clencher une nouvelle version si elles changent.\n",
    "# Note d'Architecte : On NE met PAS 'last_modified_ts' dans le hash, car on veut dÃ©tecter les changements de CONTENU mÃ©tier.\n",
    "columns_to_hash = [\n",
    "    \"product_name\", \"brands\", \"categories\", \"countries_tags\",\n",
    "    \"nutriscore_grade\", \"nova_group\", \"ecoscore_grade\",\n",
    "    \"energy_kcal_100g\", \"sugars_100g\", \"salt_100g\", \"proteins_100g\"\n",
    "]\n",
    "\n",
    "df_hashed = df_silver.withColumn(\n",
    "    \"row_hash\",\n",
    "    sha2(concat_ws(\"||\", *[col(c) for c in columns_to_hash]), 256)\n",
    ")\n",
    "\n",
    "print(\"âœ… Hashing terminÃ©.\")\n",
    "display(df_hashed.select(\"code\", \"product_name\", \"row_hash\").limit(5))"
   ],
   "id": "e5e866993adb5982",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Calcul du Hash (Fingerprint) pour chaque produit...\n",
      "âœ… Hashing terminÃ©.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[code: string, product_name: string, row_hash: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ­ Ã‰tape 4 : Initialisation du Datamart (DDL)\n",
    "\n",
    "Avant de charger les donnÃ©es, nous devons crÃ©er la structure des tables dans MySQL.\n",
    "Nous utilisons une connexion Python directe (hors Spark) pour dÃ©finir prÃ©cisÃ©ment :\n",
    "1.  **Les ClÃ©s Primaires (PK) :** `product_sk` (Auto-incrÃ©ment) pour identifier unique une *version* de produit.\n",
    "2.  **Les Index :** Sur `code` et `row_hash` pour que les recherches (Join/Upsert) soient instantanÃ©es.\n",
    "3.  **Les Colonnes SCD2 :** `effective_from` (dÃ©but), `effective_to` (fin), `is_current` (actif)."
   ],
   "id": "140f6e3f8f4c2875"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:23:15.637529Z",
     "start_time": "2026-01-18T16:23:15.456635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import mysql.connector\n",
    "from etl.shared.config import MYSQL_CONFIG\n",
    "\n",
    "def init_datamart():\n",
    "    print(\"â³ Initialisation et Tuning MySQL...\")\n",
    "\n",
    "    conn = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        port=3306,\n",
    "        user=MYSQL_CONFIG[\"user\"],\n",
    "        password=MYSQL_CONFIG[\"password\"],\n",
    "        database=\"openfoodfacts\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # --- TUNING ---\n",
    "    cursor.execute(\"SET GLOBAL max_allowed_packet=67108864\")\n",
    "\n",
    "    # --- NETTOYAGE ---\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS fact_nutrition_snapshot\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dim_product\")\n",
    "\n",
    "    # --- DIMENSION (Pas de changement) ---\n",
    "    product_ddl = \"\"\"\n",
    "    CREATE TABLE dim_product (\n",
    "        product_sk INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        code VARCHAR(255) NOT NULL,\n",
    "        product_name TEXT,\n",
    "        brands TEXT,\n",
    "        categories TEXT,\n",
    "        row_hash CHAR(64) NOT NULL,\n",
    "        effective_from DATETIME,\n",
    "        effective_to DATETIME,\n",
    "        is_current BOOLEAN,\n",
    "        INDEX idx_code (code),\n",
    "        INDEX idx_hash (row_hash)\n",
    "    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n",
    "    \"\"\"\n",
    "    cursor.execute(product_ddl)\n",
    "    print(\"   - Table 'dim_product' crÃ©Ã©e.\")\n",
    "\n",
    "    # --- FAITS (CORRECTION ICI) ---\n",
    "    # On passe les scores en VARCHAR(50) pour accepter les valeurs \"sales\" (ex: \"unknown\")\n",
    "    fact_ddl = \"\"\"\n",
    "    CREATE TABLE fact_nutrition_snapshot (\n",
    "        fact_sk INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        product_sk INT NOT NULL,\n",
    "        date_sk INT NOT NULL,\n",
    "\n",
    "        nutriscore_grade VARCHAR(50), -- CORRECTION : CHAR(1) -> VARCHAR(50)\n",
    "        ecoscore_grade VARCHAR(50),   -- CORRECTION : CHAR(1) -> VARCHAR(50)\n",
    "        nova_group INT,\n",
    "        energy_kcal_100g FLOAT,\n",
    "        sugars_100g FLOAT,\n",
    "        salt_100g FLOAT,\n",
    "        proteins_100g FLOAT,\n",
    "\n",
    "        FOREIGN KEY (product_sk) REFERENCES dim_product(product_sk)\n",
    "    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n",
    "    \"\"\"\n",
    "    cursor.execute(fact_ddl)\n",
    "    print(\"   - Table 'fact_nutrition_snapshot' crÃ©Ã©e (avec VARCHAR).\")\n",
    "\n",
    "    conn.close()\n",
    "    print(\"âœ… Datamart prÃªt.\")\n",
    "\n",
    "try:\n",
    "    init_datamart()\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur MySQL : {e}\")"
   ],
   "id": "457716d3713617bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Initialisation et Tuning MySQL...\n",
      "   - Table 'dim_product' crÃ©Ã©e.\n",
      "   - Table 'fact_nutrition_snapshot' crÃ©Ã©e (avec VARCHAR).\n",
      "âœ… Datamart prÃªt.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸšš Ã‰tape 5 : Chargement de la Dimension Produit (Initial Load)\n",
    "\n",
    "Nous sÃ©parons les donnÃ©es en deux flux :\n",
    "1.  **Dimension (`dim_product`) :** Contient les descriptions et l'historique.\n",
    "2.  **Faits (`fact_nutrition_snapshot`) :** Contient les chiffres.\n",
    "\n",
    "Ici, nous chargeons la dimension.\n",
    "* **Transformation :** On ne garde que les colonnes descriptives.\n",
    "* **Initialisation SCD2 :** Comme c'est le premier chargement, on fixe :\n",
    "    * `effective_from` = La date de modification du produit (`last_modified_ts`).\n",
    "    * `effective_to` = '9999-12-31' (Date infinie = produit actif).\n",
    "    * `is_current` = True.\n",
    "* **Ã‰criture JDBC :** On pousse vers MySQL en mode `append`."
   ],
   "id": "a9bd8a8e24cc9e58"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:23:32.281336Z",
     "start_time": "2026-01-18T16:23:15.638288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from etl.shared.config import MYSQL_CONFIG\n",
    "\n",
    "print(\"â³ PrÃ©paration de la dimension Produit...\")\n",
    "\n",
    "# SÃ©lection finale\n",
    "df_dim_product_init = df_hashed.select(\n",
    "    col(\"code\"),\n",
    "    col(\"product_name\"),\n",
    "    col(\"brands\"),\n",
    "    col(\"categories\"),\n",
    "    col(\"row_hash\"),\n",
    "    col(\"last_modified_ts\").alias(\"effective_from\"),\n",
    "    lit(\"9999-12-31 23:59:59\").cast(\"timestamp\").alias(\"effective_to\"),\n",
    "    lit(True).alias(\"is_current\")\n",
    ")\n",
    "\n",
    "# Config JDBC optimisÃ©e\n",
    "jdbc_url = MYSQL_CONFIG[\"url\"]\n",
    "jdbc_props = {\n",
    "    \"user\": MYSQL_CONFIG[\"user\"],\n",
    "    \"password\": MYSQL_CONFIG[\"password\"],\n",
    "    \"driver\": MYSQL_CONFIG[\"driver\"],\n",
    "\n",
    "    # --- CORRECTION 3 : PETITS PAQUETS ---\n",
    "    # On rÃ©duit Ã  1000 pour mÃ©nager le rÃ©seau et la mÃ©moire\n",
    "    \"batchsize\": \"1000\"\n",
    "}\n",
    "\n",
    "print(\"ðŸš€ Ã‰criture dans MySQL (dim_product)...\")\n",
    "\n",
    "try:\n",
    "    df_dim_product_init.write \\\n",
    "        .jdbc(url=jdbc_url, table=\"dim_product\", mode=\"append\", properties=jdbc_props)\n",
    "\n",
    "    print(\"âœ… Chargement terminÃ© avec succÃ¨s !\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur d'Ã©criture : {e}\")"
   ],
   "id": "63a2f39efea90142",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ PrÃ©paration de la dimension Produit...\n",
      "ðŸš€ Ã‰criture dans MySQL (dim_product)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:======================================================>(492 + 5) / 497]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Chargement terminÃ© avec succÃ¨s !\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ðŸ“Š Ã‰tape 6 : Chargement de la Table de Faits (Fact Table)\n",
    "\n",
    "C'est l'Ã©tape finale. Nous devons charger les mesures (sucre, sel, nutriscore...) dans `fact_nutrition_snapshot`.\n",
    "**Le dÃ©fi :** La table de faits a besoin de la clÃ© Ã©trangÃ¨re `product_sk`. Or, cette clÃ© a Ã©tÃ© gÃ©nÃ©rÃ©e par MySQL (Auto-increment) Ã  l'Ã©tape prÃ©cÃ©dente. Spark ne la connait pas.\n",
    "\n",
    "**StratÃ©gie :**\n",
    "1.  **Lecture (Lookup) :** On lit la table `dim_product` depuis MySQL pour rÃ©cupÃ©rer le couple `(code, product_sk)`.\n",
    "2.  **Jointure :** On joint ces IDs avec notre DataFrame Spark (`df_hashed`) sur le code-barre.\n",
    "3.  **Calcul Date Key :** On transforme la date en entier `YYYYMMDD` (ex: `20230520`) pour la clÃ© de temps `date_sk`.\n",
    "4.  **Ã‰criture :** On insÃ¨re les lignes dans `fact_nutrition_snapshot`."
   ],
   "id": "799e79f9cbc0518"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-18T16:24:05.149274Z",
     "start_time": "2026-01-18T16:23:32.293915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "print(\"â³ Chargement des Faits : RÃ©cupÃ©ration des IDs MySQL...\")\n",
    "\n",
    "# 1. On relit la dimension depuis MySQL pour avoir les product_sk gÃ©nÃ©rÃ©s\n",
    "df_dim_mysql = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", MYSQL_CONFIG[\"url\"]) \\\n",
    "    .option(\"dbtable\", \"dim_product\") \\\n",
    "    .option(\"user\", MYSQL_CONFIG[\"user\"]) \\\n",
    "    .option(\"password\", MYSQL_CONFIG[\"password\"]) \\\n",
    "    .option(\"driver\", MYSQL_CONFIG[\"driver\"]) \\\n",
    "    .load() \\\n",
    "    .select(\"product_sk\", \"code\") # On a juste besoin de la clÃ© de jointure et de la PK\n",
    "\n",
    "# 2. Jointure : Spark (DonnÃ©es) + MySQL (IDs)\n",
    "# On utilise le 'code' (EAN) comme pivot pour retrouver l'ID technique\n",
    "df_facts = df_hashed.join(df_dim_mysql, on=\"code\", how=\"inner\")\n",
    "\n",
    "# 3. PrÃ©paration finale des colonnes de la Fact Table\n",
    "df_facts_final = df_facts.select(\n",
    "    col(\"product_sk\"), # La clÃ© Ã©trangÃ¨re rÃ©cupÃ©rÃ©e de MySQL\n",
    "\n",
    "    # CrÃ©ation d'une clÃ© de date simple (YYYYMMDD) basÃ©e sur la date de modif\n",
    "    date_format(col(\"last_modified_ts\"), \"yyyyMMdd\").cast(\"integer\").alias(\"date_sk\"),\n",
    "\n",
    "    # Les Mesures\n",
    "    col(\"nutriscore_grade\"),\n",
    "    col(\"ecoscore_grade\"),\n",
    "    col(\"nova_group\"),\n",
    "    col(\"energy_kcal_100g\"),\n",
    "    col(\"sugars_100g\"),\n",
    "    col(\"salt_100g\"),\n",
    "    col(\"proteins_100g\")\n",
    ")\n",
    "\n",
    "print(f\"ðŸ“¦ PrÃªt Ã  charger {df_facts_final.count():,} lignes de faits.\")\n",
    "\n",
    "# 4. Ã‰criture dans MySQL\n",
    "print(\"ðŸš€ Ã‰criture dans 'fact_nutrition_snapshot'...\")\n",
    "try:\n",
    "    df_facts_final.write \\\n",
    "        .jdbc(\n",
    "            url=MYSQL_CONFIG[\"url\"],\n",
    "            table=\"fact_nutrition_snapshot\",\n",
    "            mode=\"append\",\n",
    "            properties=jdbc_props # On rÃ©utilise les props avec batchsize=1000\n",
    "        )\n",
    "    print(\"âœ… TERMINE ! Le Datamart est complet (Dimensions + Faits).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Erreur Ã©criture Faits : {e}\")"
   ],
   "id": "da592a55ea81c89c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â³ Chargement des Faits : RÃ©cupÃ©ration des IDs MySQL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¦ PrÃªt Ã  charger 425,590 lignes de faits.\n",
      "ðŸš€ Ã‰criture dans 'fact_nutrition_snapshot'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:===================================================>    (12 + 1) / 13]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… TERMINE ! Le Datamart est complet (Dimensions + Faits).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
