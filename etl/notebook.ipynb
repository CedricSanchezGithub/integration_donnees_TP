{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## TRDE703 Atelier Int√©gration des Donn√©es",
   "id": "fd7d5d2acdc3555b"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. D'ABORD : ON CONFIGURE LES CHEMINS (Le \"Hack\") ---\n",
    "# On doit le faire AVANT d'importer 'etl.*' sinon Python ne trouve pas le dossier.\n",
    "\n",
    "current_dir = Path(os.getcwd())\n",
    "# Si le notebook est dans 'etl/', la racine est le dossier parent\n",
    "project_root = current_dir.parent if current_dir.name == \"etl\" else current_dir\n",
    "\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "print(f\"‚úÖ Racine ajout√©e au path : {project_root}\")\n",
    "\n",
    "# --- 2. ENSUITE : ON PEUT IMPORTER TON CODE ---\n",
    "from pyspark.sql import SparkSession\n",
    "from etl.shared.config import SPARK_CONFIG  # Maintenant √ßa va marcher !\n",
    "\n",
    "# --- 3. ENFIN : ON LANCE SPARK AVEC LE JAR ---\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "builder = SparkSession.builder\n",
    "for key, val in SPARK_CONFIG.items():\n",
    "    builder = builder.config(key, val)\n",
    "\n",
    "spark = builder.getOrCreate()\n",
    "\n",
    "print(f\"‚úÖ Session Spark cr√©√©e avec le JAR : {SPARK_CONFIG.get('spark.jars')}\")"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from etl.shared.config import SPARK_CONFIG, MYSQL_CONFIG\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "print(\"‚öôÔ∏è Configuration charg√©e avec succ√®s.\")"
   ],
   "id": "ef2390298c38f5a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# On construit le chemin proprement avec pathlib\n",
    "# project_root est d√©j√† d√©fini dans la premi√®re cellule\n",
    "json_filepath = str(project_root / \"data\" / \"raw\" / \"openfoodfacts-products.jsonl\")\n",
    "\n",
    "print(f\"üìÇ Fichier cible : {json_filepath}\")\n",
    "\n",
    "if os.path.exists(json_filepath):\n",
    "    print(\"‚úÖ Le fichier existe bien.\")\n",
    "else:\n",
    "    print(\"‚ùå Fichier introuvable. V√©rifie le dossier data/raw/\")"
   ],
   "id": "cf7e6bd3f66a8281",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### üí° Un mot sur nos choix (et les consignes du TP)\n",
    "\n",
    "Pourquoi s'emb√™ter √† √©crire ce sch√©ma manuellement ?\n",
    "\n",
    "1.  **Respect de la consigne :** Le sujet est strict : *\"Lecture JSON/CSV avec sch√©ma explicite (pas d'inf√©rence magique en prod)\"*. Utiliser `inferSchema=True` nous ferait perdre des points.\n",
    "2.  **Gestion de l'Historique (SCD2) :** Le sujet impose de g√©rer le *\"SCD2 produit\"*. Pour cela, nous avons imp√©rativement besoin du timestamp brut (`last_modified_t` en `LongType`) pour comparer les versions √† la seconde pr√®s.\n",
    "3.  **Structure Imbriqu√©e :** Le format JSONL groupe les nutriments dans un objet. Notre sch√©ma refl√®te cette r√©alit√© (`StructType` imbriqu√©) pour √©viter de cr√©er 1000 colonnes plates inutiles.\n",
    "4.  **S√©curit√© (`String`) :** Pour des champs instables comme `nova_group`, on lit en `String` pour √©viter que Spark ne rejette la ligne en cas de format inattendu."
   ],
   "id": "da340475acb067a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType, LongType, ArrayType\n",
    "\n",
    "def get_jsonl_schema():\n",
    "    \"\"\"\n",
    "    Sch√©ma robuste pour l'ingestion JSONL.\n",
    "    G√®re les types imbriqu√©s (nested) propres √† MongoDB/JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    # D√©finition de la sous-structure pour les nutriments\n",
    "    # (Permet de lire l'objet \"nutriments\": { ... } proprement)\n",
    "    nutriments_schema = StructType([\n",
    "        StructField(\"energy-kcal_100g\", FloatType(), True),\n",
    "        StructField(\"sugars_100g\", FloatType(), True),\n",
    "        StructField(\"salt_100g\", FloatType(), True),\n",
    "        StructField(\"sodium_100g\", FloatType(), True),\n",
    "        StructField(\"fiber_100g\", FloatType(), True),\n",
    "        StructField(\"proteins_100g\", FloatType(), True)\n",
    "    ])\n",
    "\n",
    "    return StructType([\n",
    "        # --- Identifiants & M√©tadonn√©es (Crucial pour SCD2) ---\n",
    "        StructField(\"code\", StringType(), True),            # La cl√© primaire m√©tier (EAN)\n",
    "        StructField(\"product_name\", StringType(), True),\n",
    "        StructField(\"last_modified_t\", LongType(), True),   # Timestamp UNIX (versioning)\n",
    "        StructField(\"created_t\", LongType(), True),\n",
    "\n",
    "        # --- Dimensions (Marques, Cat√©gories...) ---\n",
    "        StructField(\"brands\", StringType(), True),\n",
    "        StructField(\"categories\", StringType(), True),\n",
    "        StructField(\"countries_tags\", ArrayType(StringType()), True), # Liste de pays [\"en:france\", \"en:belgium\"]\n",
    "\n",
    "        # --- Qualit√© & Scores ---\n",
    "        StructField(\"nutriscore_grade\", StringType(), True),\n",
    "        StructField(\"nova_group\", IntegerType(), True),     # Souvent un entier dans le JSON\n",
    "        StructField(\"ecoscore_grade\", StringType(), True),\n",
    "\n",
    "        # --- Mesures (Imbriqu√©es) ---\n",
    "        StructField(\"nutriments\", nutriments_schema, True)  # L'objet imbriqu√©\n",
    "    ])\n",
    "\n",
    "print(\"‚úÖ Sch√©ma JSONL d√©fini.\")"
   ],
   "id": "69b0cb1c6a8e6464",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Adapte le nom du fichier si n√©cessaire\n",
    "input_file = \"openfoodfacts-products.jsonl\"\n",
    "raw_path = str(project_root / \"data\" / \"raw\" / input_file)\n",
    "\n",
    "print(f\"üìÇ Pr√©paration de la lecture : {raw_path}\")\n",
    "\n",
    "try:\n",
    "    # 1. Lecture \"Lazy\" (Paresseuse)\n",
    "    # Spark ne lit rien pour l'instant, il note juste le plan d'action.\n",
    "    df_raw = spark.read \\\n",
    "        .schema(get_jsonl_schema()) \\\n",
    "        .json(raw_path)\n",
    "\n",
    "    print(\"‚úÖ Lecture configur√©e (Lazy). Le chargement r√©el se fera apr√®s le sampling.\")\n",
    "\n",
    "\n",
    "    count = df_raw.count()\n",
    "    print(f\"üìä Nombre de produits ing√©r√©s : {count:,}\")\n",
    "\n",
    "    df_raw.printSchema()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur de lecture : {e}\")"
   ],
   "id": "6fa16efe239268b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üßπ √âtape 2 : Transformation \"Silver\" (Nettoyage & Typage)\n",
    "\n",
    "Maintenant que les donn√©es brutes sont charg√©es, nous devons les rendre utilisables pour l'analyse et le SCD2.\n",
    "Cette √©tape applique les r√®gles de qualit√© demand√©es :\n",
    "\n",
    "1.  **Typage Temporel :** Conversion des timestamps UNIX (`Long`) en vraies dates (`Timestamp`) pour `last_modified_t` et `created_t`.\n",
    "2.  **Nettoyage Textuel :** Suppression des espaces superflus (`trim`) sur les codes-barres et noms.\n",
    "3.  **Extraction des Nutriments :** Aplatissement de la structure imbriqu√©e `nutriments` pour faciliter les requ√™tes SQL futures.\n",
    "4.  **Gestion des Nulls :** Conversion s√©curis√©e de `nova_group` (texte vers entier) et filtrage des produits sans code-barre."
   ],
   "id": "9f02f9f8021945a5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import col, trim, from_unixtime, to_timestamp, when\n",
    "\n",
    "print(\"‚è≥ D√©marrage du nettoyage Silver avec Sampling...\")\n",
    "\n",
    "# --- CORRECTION 1 : ECHANTILLONNAGE (SAMPLING) ---\n",
    "# On ne garde que 10% des donn√©es (environ 400k lignes) pour sauver ton disque dur.\n",
    "# seed=42 permet d'avoir toujours les m√™mes 10% si tu relances.\n",
    "df_sampled = df_raw.sample(withReplacement=False, fraction=0.1, seed=42)\n",
    "\n",
    "df_silver = df_sampled \\\n",
    "    .select(\n",
    "        # --- 1. Nettoyage des Cl√©s & Textes ---\n",
    "        trim(col(\"code\")).alias(\"code\"),\n",
    "        trim(col(\"product_name\")).alias(\"product_name\"),\n",
    "\n",
    "        # --- 2. Gestion Temporelle ---\n",
    "        from_unixtime(col(\"last_modified_t\")).cast(\"timestamp\").alias(\"last_modified_ts\"),\n",
    "        from_unixtime(col(\"created_t\")).cast(\"timestamp\").alias(\"created_ts\"),\n",
    "\n",
    "        # --- 3. Normalisation des Dimensions ---\n",
    "        col(\"countries_tags\"),\n",
    "        trim(col(\"brands\")).alias(\"brands\"),\n",
    "        trim(col(\"categories\")).alias(\"categories\"),\n",
    "\n",
    "        # --- 4. Qualit√© & Scores ---\n",
    "        trim(col(\"nutriscore_grade\")).alias(\"nutriscore_grade\"),\n",
    "        trim(col(\"ecoscore_grade\")).alias(\"ecoscore_grade\"),\n",
    "        col(\"nova_group\").cast(\"integer\").alias(\"nova_group\"),\n",
    "\n",
    "        # --- 5. Nutriments ---\n",
    "        col(\"nutriments.energy-kcal_100g\").alias(\"energy_kcal_100g\"),\n",
    "        col(\"nutriments.sugars_100g\").alias(\"sugars_100g\"),\n",
    "        col(\"nutriments.salt_100g\").alias(\"salt_100g\"),\n",
    "        col(\"nutriments.proteins_100g\").alias(\"proteins_100g\")\n",
    "    ) \\\n",
    "    .filter(col(\"code\").isNotNull()) \\\n",
    "    .filter(col(\"code\") != \"\")\n",
    "\n",
    "# On met en cache ce petit √©chantillon\n",
    "df_silver.cache()\n",
    "\n",
    "count = df_silver.count()\n",
    "print(f\"‚úÖ Nettoyage termin√© sur l'√©chantillon. Produits restants : {count:,}\")\n",
    "print(\"(C'est normal d'en avoir moins, on a pris 10% volontairement !)\")\n",
    "\n",
    "display(df_silver.select(\"code\", \"last_modified_ts\", \"product_name\").limit(5))"
   ],
   "id": "2a765f28173c7f65",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üîê √âtape 3 : Fingerprinting (Pr√©paration SCD2)\n",
    "\n",
    "Pour g√©rer l'historique (SCD2) efficacement, nous ne pouvons pas comparer toutes les colonnes √† chaque fois.\n",
    "Nous allons g√©n√©rer un **Hash Technique (`row_hash`)** : une empreinte digitale unique bas√©e sur les colonnes m√©tier.\n",
    "\n",
    "* **Strat√©gie :** On concat√®ne toutes les colonnes importantes (Nom, Marque, Nutriscore, Sucre...) et on applique un hachage SHA-256.\n",
    "* **Int√©r√™t :** Si le hash change, cela signifie que le produit a √©t√© modifi√©. C'est ce qui d√©clenchera la cr√©ation d'une nouvelle version dans le Datamart."
   ],
   "id": "ffd1bf078dba36b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import sha2, concat_ws, col\n",
    "\n",
    "print(\"‚è≥ Calcul du Hash (Fingerprint) pour chaque produit...\")\n",
    "\n",
    "# On s√©lectionne les colonnes qui doivent d√©clencher une nouvelle version si elles changent.\n",
    "# Note d'Architecte : On NE met PAS 'last_modified_ts' dans le hash, car on veut d√©tecter les changements de CONTENU m√©tier.\n",
    "columns_to_hash = [\n",
    "    \"product_name\", \"brands\", \"categories\", \"countries_tags\",\n",
    "    \"nutriscore_grade\", \"nova_group\", \"ecoscore_grade\",\n",
    "    \"energy_kcal_100g\", \"sugars_100g\", \"salt_100g\", \"proteins_100g\"\n",
    "]\n",
    "\n",
    "df_hashed = df_silver.withColumn(\n",
    "    \"row_hash\",\n",
    "    sha2(concat_ws(\"||\", *[col(c) for c in columns_to_hash]), 256)\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Hashing termin√©.\")\n",
    "display(df_hashed.select(\"code\", \"product_name\", \"row_hash\").limit(5))"
   ],
   "id": "e5e866993adb5982",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üè≠ √âtape 4 : Initialisation du Datamart (DDL)\n",
    "\n",
    "Avant de charger les donn√©es, nous devons cr√©er la structure des tables dans MySQL.\n",
    "Nous utilisons une connexion Python directe (hors Spark) pour d√©finir pr√©cis√©ment :\n",
    "1.  **Les Cl√©s Primaires (PK) :** `product_sk` (Auto-incr√©ment) pour identifier unique une *version* de produit.\n",
    "2.  **Les Index :** Sur `code` et `row_hash` pour que les recherches (Join/Upsert) soient instantan√©es.\n",
    "3.  **Les Colonnes SCD2 :** `effective_from` (d√©but), `effective_to` (fin), `is_current` (actif)."
   ],
   "id": "140f6e3f8f4c2875"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import mysql.connector\n",
    "from etl.shared.config import MYSQL_CONFIG\n",
    "\n",
    "def init_datamart():\n",
    "    print(\"‚è≥ Initialisation et Tuning MySQL...\")\n",
    "\n",
    "    conn = mysql.connector.connect(\n",
    "        host=\"localhost\",\n",
    "        port=3306,\n",
    "        user=MYSQL_CONFIG[\"user\"],\n",
    "        password=MYSQL_CONFIG[\"password\"],\n",
    "        database=\"openfoodfacts\"\n",
    "    )\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # --- CORRECTION 2 : TUNING MYSQL (CRITIQUE) ---\n",
    "    # On autorise des paquets jusqu'√† 64 Mo pour √©viter le \"PacketTooBigException\"\n",
    "    cursor.execute(\"SET GLOBAL max_allowed_packet=67108864\")\n",
    "    print(\"   - Option MySQL 'max_allowed_packet' pass√©e √† 64MB.\")\n",
    "\n",
    "    # Suppression (Faits d'abord, Dimensions ensuite)\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS fact_nutrition_snapshot\")\n",
    "    cursor.execute(\"DROP TABLE IF EXISTS dim_product\")\n",
    "\n",
    "    # Cr√©ation Dimension (avec TEXT pour les champs longs)\n",
    "    product_ddl = \"\"\"\n",
    "    CREATE TABLE dim_product (\n",
    "        product_sk INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        code VARCHAR(255) NOT NULL,\n",
    "\n",
    "        product_name TEXT, -- TEXT car VARCHAR(500) est trop court pour certains produits\n",
    "        brands TEXT,       -- TEXT car les listes de marques peuvent √™tre immenses\n",
    "        categories TEXT,\n",
    "\n",
    "        row_hash CHAR(64) NOT NULL,\n",
    "        effective_from DATETIME,\n",
    "        effective_to DATETIME,\n",
    "        is_current BOOLEAN,\n",
    "\n",
    "        INDEX idx_code (code),\n",
    "        INDEX idx_hash (row_hash)\n",
    "    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n",
    "    \"\"\"\n",
    "    cursor.execute(product_ddl)\n",
    "    print(\"   - Table 'dim_product' cr√©√©e (avec TEXT).\")\n",
    "\n",
    "    # Cr√©ation Faits\n",
    "    fact_ddl = \"\"\"\n",
    "    CREATE TABLE fact_nutrition_snapshot (\n",
    "        fact_sk INT AUTO_INCREMENT PRIMARY KEY,\n",
    "        product_sk INT NOT NULL,\n",
    "        date_sk INT NOT NULL,\n",
    "\n",
    "        nutriscore_grade CHAR(1),\n",
    "        ecoscore_grade CHAR(1),\n",
    "        nova_group INT,\n",
    "        energy_kcal_100g FLOAT,\n",
    "        sugars_100g FLOAT,\n",
    "        salt_100g FLOAT,\n",
    "        proteins_100g FLOAT,\n",
    "\n",
    "        FOREIGN KEY (product_sk) REFERENCES dim_product(product_sk)\n",
    "    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;\n",
    "    \"\"\"\n",
    "    cursor.execute(fact_ddl)\n",
    "    print(\"   - Table 'fact_nutrition_snapshot' cr√©√©e.\")\n",
    "\n",
    "    conn.close()\n",
    "    print(\"‚úÖ Datamart pr√™t.\")\n",
    "\n",
    "try:\n",
    "    init_datamart()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur MySQL : {e}\")"
   ],
   "id": "457716d3713617bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## üöö √âtape 5 : Chargement de la Dimension Produit (Initial Load)\n",
    "\n",
    "Nous s√©parons les donn√©es en deux flux :\n",
    "1.  **Dimension (`dim_product`) :** Contient les descriptions et l'historique.\n",
    "2.  **Faits (`fact_nutrition_snapshot`) :** Contient les chiffres.\n",
    "\n",
    "Ici, nous chargeons la dimension.\n",
    "* **Transformation :** On ne garde que les colonnes descriptives.\n",
    "* **Initialisation SCD2 :** Comme c'est le premier chargement, on fixe :\n",
    "    * `effective_from` = La date de modification du produit (`last_modified_ts`).\n",
    "    * `effective_to` = '9999-12-31' (Date infinie = produit actif).\n",
    "    * `is_current` = True.\n",
    "* **√âcriture JDBC :** On pousse vers MySQL en mode `append`."
   ],
   "id": "a9bd8a8e24cc9e58"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from etl.shared.config import MYSQL_CONFIG\n",
    "\n",
    "print(\"‚è≥ Pr√©paration de la dimension Produit...\")\n",
    "\n",
    "# S√©lection finale\n",
    "df_dim_product_init = df_hashed.select(\n",
    "    col(\"code\"),\n",
    "    col(\"product_name\"),\n",
    "    col(\"brands\"),\n",
    "    col(\"categories\"),\n",
    "    col(\"row_hash\"),\n",
    "    col(\"last_modified_ts\").alias(\"effective_from\"),\n",
    "    lit(\"9999-12-31 23:59:59\").cast(\"timestamp\").alias(\"effective_to\"),\n",
    "    lit(True).alias(\"is_current\")\n",
    ")\n",
    "\n",
    "# Config JDBC optimis√©e\n",
    "jdbc_url = MYSQL_CONFIG[\"url\"]\n",
    "jdbc_props = {\n",
    "    \"user\": MYSQL_CONFIG[\"user\"],\n",
    "    \"password\": MYSQL_CONFIG[\"password\"],\n",
    "    \"driver\": MYSQL_CONFIG[\"driver\"],\n",
    "\n",
    "    # --- CORRECTION 3 : PETITS PAQUETS ---\n",
    "    # On r√©duit √† 1000 pour m√©nager le r√©seau et la m√©moire\n",
    "    \"batchsize\": \"1000\"\n",
    "}\n",
    "\n",
    "print(\"üöÄ √âcriture dans MySQL (dim_product)...\")\n",
    "\n",
    "try:\n",
    "    df_dim_product_init.write \\\n",
    "        .jdbc(url=jdbc_url, table=\"dim_product\", mode=\"append\", properties=jdbc_props)\n",
    "\n",
    "    print(\"‚úÖ Chargement termin√© avec succ√®s !\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erreur d'√©criture : {e}\")"
   ],
   "id": "63a2f39efea90142",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
