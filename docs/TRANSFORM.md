# üìÇ Documentation : etl/transform.py

### üìÑ En bref
Ce fichier est l'usine de recyclage du projet : il prend les donn√©es brutes, les nettoie, retire les doublons, calcule des empreintes num√©riques (hash) pour d√©tecter les modifications et pr√©pare les diff√©rentes tables pour la base de donn√©es.

---

### üéØ Pourquoi ce fichier ?
Les donn√©es brutes extraites (√©tape *Extract*) sont rarement utilisables telles quelles :
1.  **Elles sont "sales"** : Espaces en trop, types incorrects (dates en chiffres bizarres), valeurs manquantes.
2.  **Elles contiennent des doublons** : Un m√™me produit peut appara√Ætre plusieurs fois si le fichier source a √©t√© mis √† jour.
3.  **Elles ne sont pas relationnelles** : Les cat√©gories sont souvent une longue liste de texte ("Chips, Snacks, Sal√©") qu'il faut d√©couper pour les analyser proprement.

Ce fichier transforme le "Bronze" (donn√©e brute) en "Silver" (donn√©e propre) et pr√©pare le "Gold" (donn√©e pr√™te √† l'analyse).

---

### ‚öôÔ∏è Comment √ßa marche ?

Le code est d√©coup√© en plusieurs fonctions sp√©cialis√©es :

#### 1. Le grand nettoyage (`clean_data`)
C'est la premi√®re √©tape obligatoire.
* **Typage** : On transforme les dates (format "Unix timestamp") en vraies dates lisibles.
* **Extraction** : On va chercher les nutriments (sucre, sel, etc.) qui √©taient cach√©s dans des sous-structures du fichier.
* **D√©duplication intelligente** : Si le produit "Nutella" appara√Æt 3 fois, on utilise une "fen√™tre" (`Window`) pour ne garder que la version la plus r√©cente (celle avec le `last_modified_t` le plus grand).

#### 2. La signature num√©rique (`add_technical_hash`)
Pour g√©rer l'historisation (SCD2), on a besoin de savoir si un produit a chang√©.
Au lieu de comparer les 50 colonnes une par une (ce qui est lent), on colle toutes les valeurs importantes ensemble et on calcule un **Hash (SHA256)**. C'est une empreinte digitale unique : si une seule virgule change dans le produit, le Hash change radicalement.

#### 3. La gestion des cat√©gories (`extract_unique_categories` & `prepare_bridge_table`)
Dans le fichier source, un produit a une liste de cat√©gories : "Boissons, Sucr√©, Sodas".
* **Explosion** : La fonction `explode` fait "√©clater" cette liste. Le produit se retrouve dupliqu√© pour chaque cat√©gorie.
* **Table Bridge** : On cr√©e une table de liaison qui dit simplement "Le Produit X est li√© √† la Cat√©gorie Y". C'est indispensable pour faire des filtres pr√©cis plus tard.

#### 4. La pr√©paration finale (`prepare_fact_table`)
On pr√©pare la table centrale d'analyse (Table de Faits). On nettoie les valeurs aberrantes (comme les "Infinity" qui font planter les calculs) pour les remplacer par du vide (`NULL`), garantissant des graphiques propres √† la fin.

---

### üí° Le coin de l'expert

**Pourquoi d√©doublonner avec une `Window` et `row_number()` ?**

Tu verras souvent `df.dropDuplicates()` dans les tutos Spark. C'est bien, mais dangereux : cela garde une ligne *au hasard* parmi les doublons.
Ici, on utilise une m√©thode pro :
1.  On groupe par code produit.
2.  On trie par date de modification d√©croissante.
3.  On num√©rote les lignes (1, 2, 3...).
4.  On ne garde que la n¬∞1.
Cela garantit math√©matiquement qu'on conserve toujours la **derni√®re version** des donn√©es.