# üìÇ Documentation : etl/extract.py

### üìÑ En bref
Ce fichier est le point d'entr√©e des donn√©es : il lit le fichier brut **OpenFoodFacts** (format JSONL) en appliquant une grille de lecture stricte pour transformer ce texte en un tableau manipulable (DataFrame Spark).

---

### üéØ Pourquoi ce fichier ?
Dans un projet Data, l'√©tape d'**Extraction** est critique. On ne peut pas simplement "ouvrir" un fichier de plusieurs giga-octets comme un Excel classique.
Ce script a deux r√¥les majeurs :
1.  **Charger les donn√©es** depuis le disque dur vers le moteur de calcul (Spark).
2.  **Imposer une structure** d√®s le d√©but. Les fichiers JSON sont "flexibles" (parfois une colonne existe, parfois non). Ici, on d√©finit exactement quelles informations nous int√©ressent pour ne pas perdre de temps √† tout charger.

---

### ‚öôÔ∏è Comment √ßa marche ?

Le code se divise en deux √©tapes logiques :

#### √âtape 1 : D√©finir la carte d'identit√© des donn√©es (`get_jsonl_schema`)
Avant de lire le fichier, on pr√©vient Spark de ce qu'il va trouver. On liste les colonnes attendues et leur type (texte, nombre entier, nombre √† virgule...).
* **Exemple** : On pr√©cise que `code` est une cha√Æne de caract√®res et que `additives_n` (nombre d'additifs) est un entier.
* **Le cas sp√©cial** : Les nutriments (sucre, sel, √©nergie) sont regroup√©s dans une "bo√Æte" √† l'int√©rieur du JSON. On d√©finit donc une sous-structure (`nutriments_schema`) pour aller chercher ces infos imbriqu√©es proprement.

#### √âtape 2 : L'extraction proprement dite (`extract_data`)
C'est la fonction qui fait le travail :
1.  **Localisation** : Elle construit le chemin d'acc√®s vers le dossier `data/raw` o√π est stock√© le fichier.
2.  **Lecture optimis√©e** : Elle demande √† Spark de lire le fichier en utilisant le **sch√©ma** d√©fini √† l'√©tape 1.
3.  **S√©curit√©** : Si le fichier est introuvable ou illisible, elle capture l'erreur et affiche un message clair au lieu de faire planter tout le programme silencieusement.

---

### üí° Le coin de l'expert

**Pourquoi d√©finir un schema strict (`StructType`) au lieu de laisser Spark deviner ?**

C'est une astuce de performance majeure.
Par d√©faut, Spark utilise le `schema inference` : il doit lire **tout le fichier une premi√®re fois** juste pour deviner si la colonne "sucre" contient des chiffres ou du texte, puis le relire pour charger les donn√©es.
Sur un fichier massif comme OpenFoodFacts (plusieurs Go), cela doublerait le temps de chargement ! En lui donnant le sch√©ma, Spark lit le fichier une seule fois. Gain de temps imm√©diat. üöÄ