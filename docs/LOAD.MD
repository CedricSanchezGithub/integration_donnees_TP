# üìÇ Documentation : etl/load.py

### üìÑ En bref
C'est le **livreur** du projet : ce fichier prend les donn√©es transform√©es et les d√©pose proprement dans la base de donn√©es (MySQL). Il ne se contente pas de "jeter" les donn√©es, il les range m√©ticuleusement en g√©rant l'historique des changements.

---

### üéØ Pourquoi ce fichier ?
Charger des donn√©es, ce n'est pas juste faire un "copier-coller". Dans un projet Data professionnel, on a deux d√©fis majeurs r√©solus ici :
1.  **L'historisation (SCD2)** : Si la composition du Nutella change demain, on veut garder l'ancienne version pour nos statistiques pass√©es ET avoir la nouvelle. On ne doit pas √©craser l'information, mais *ajouter* une nouvelle page √† l'histoire du produit.
2.  **La coh√©rence** : On doit remplir plusieurs tables (Produits, Cat√©gories, Faits) sans casser les liens entre elles (Cl√©s √©trang√®res).

---

### ‚öôÔ∏è Comment √ßa marche ?

Le fichier contient plusieurs m√©canismes ing√©nieux :

#### 1. L'initialisation du terrain (`init_database`)
Avant de charger quoi que ce soit, le script v√©rifie si les tables existent. Si non, il les cr√©e avec le bon format (colonnes, types, index). C'est ici qu'on d√©finit l'architecture "en √©toile" (Dimension Produit, Dimension Cat√©gorie, Table de Faits).

#### 2. La gestion intelligente de l'histoire (`load_dimension_scd2`)
C'est le c≈ìur du r√©acteur.
* **Comparaison** : Il compare le "Hash" (l'empreinte) du produit qui arrive avec celui d√©j√† en base.
* **Arbitrage** :
    * *Rien n'a chang√© ?* On ne fait rien.
    * *C'est nouveau ?* On ins√®re le produit.
    * *√áa a chang√© ?* On "ferme" l'ancienne ligne (on met une date de fin `effective_to`) et on ins√®re la nouvelle ligne active.

#### 3. La technique du "Staging" (`load_facts`, `_close_products_via_staging`)
Pour mettre √† jour la base de donn√©es rapidement, Spark √©crit d'abord les donn√©es dans une table temporaire (table de "brouillon" ou *staging*).
Ensuite, une requ√™te SQL ultra-rapide d√©place les donn√©es du brouillon vers la table finale. Cela permet de faire des op√©rations complexes (comme des mises √† jour massives) que Spark g√®re mal en direct.

#### 4. La connexion robuste (`_get_db_connection`)
Si la base de donn√©es a un micro-hoquet (coupure r√©seau de 1 seconde), le script ne plante pas. Il attend un peu et r√©essaie jusqu'√† 10 fois. C'est ce qu'on appelle la r√©silience.

---

### üí° Le coin de l'expert

**Pourquoi utiliser des tables de "Staging" temporaires ?**

Tu verras des fonctions comme `_close_products_via_staging` ou `_upsert_facts_via_staging`.
Spark est tr√®s fort pour *ajouter* des donn√©es (Insert), mais tr√®s mauvais pour les *modifier* (Update) ligne par ligne.
L'astuce de pro ici est de :
1.  Utiliser Spark pour √©crire en masse dans une table temporaire (`staging_...`).
2.  Lancer une seule commande SQL (`INSERT ... ON DUPLICATE KEY UPDATE` ou `UPDATE ... JOIN`) pour appliquer les changements.
C'est souvent 10 √† 100 fois plus rapide qu'une mise √† jour classique ! üöÄ